var documenterSearchIndex = {"docs":
[{"location":"references.html","page":"References","title":"References","text":"T. Allen and C. Arkolakis. The Welfare Effects of Transportation Infrastructure Improvements. The Review of Economic Studies 89 (2022).\n\n\n\n","category":"page"},{"location":"optimal_transport.html#Example:-Optimal-Transportation-Policy","page":"Optimal Transport Example","title":"Example: Optimal Transportation Policy","text":"","category":"section"},{"location":"optimal_transport.html#Set-up","page":"Optimal Transport Example","title":"Set-up","text":"","category":"section"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Consider a city defined as a weighted graph with N nodes and a road network defined as E edges (roads) between nodes. The weight of the graph represents travel time between nodes. If nodes i and j have a road connecting them, the travel time for the direct route between the two nodes is given by t_ij. If nodes i and j do not have a road connecting them, then there is no direct route and a traveler is forced to take an indirect route (however an indirect route may still be faster than a direct route).","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"An individual lives in node i and works in node j. All home and workplace decisions are exogenous. Denote the fraction of individuals commuting from location i to location j as p_ij.","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"However an individual still chooses the route they take from home to work. That is, they choose what node-to-node-to-node route mathfrakR_ij they take between their home i and work j. ","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"We borrow the decision problem and solution for a worker's optimal route from Allen and Arkolakis [1]. In brief, workers seek the route with the shortest travel time, but also receive i.i.d. Frechet preference shocks over potential routes.","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Allen and Arkolakis [1] show that for a given i-j home-workplace pair, the effective expected travel cost faced by the worker, accounting for both travel time and the idiosyncratic preference shock, can be expressed as ","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"tau_ij = left(sum_r in mathfrakR_ij left(prod_l = 1^L t^-theta_r_l-1 r_lright)right)^-frac1theta","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Where t_r_l-1 rl corresponds to the node-to-node travel time between leg l-1 and step l on the route. ","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Moreover Allen and Arkolakis show that there is a convenient matrix expression for this travel time as well. Let mathbbA = t_ij^-theta, then define mathbfB as the Leontief inverse of A","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"mathbfB = (I - A)^-1","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"and we thus have","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"tau_ij = b_ij^-frac1theta","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"The planner cares about the total cost of travel faced by all individuals in the city. Thus their welfare function is","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Wleft(tau_ij_ijin Nright) = -left(sum_ij in Ntau_ijpi_ijright)","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Let mathbfT be an N times N matrix representing the travel time between any two nodes, and let alpha be the shape parameter of the idiosyncratic Frechet preference shock for any route.","category":"page"},{"location":"optimal_transport.html#The-Policy-Space","page":"Optimal Transport Example","title":"The Policy Space","text":"","category":"section"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"The planner has the budget to improve K  E roads between nodes by reducing travel speed on a given road. They need to choose which roads they want to upgrade. ","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"We find the best combination of roads to upgrade through the  Metropolitan-Hastings algorithm. Mapping the problem described above to the discussion previously about the Metropolitan-Hastings algorithm we have","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"mathcalN: The policy space. This is a list of K roads to improve. Note that this policy space is very large, exactly N text Choose  K. If you have 40 roads and have the budget to improve 10 of them, you have approximately 848 million ways to do that. \nPsi(N N): The initial guess for a Markov Chain. Because the  policy space is so large, we don't characterize Psi directly,  and instead think of a method for drawing N given N in a way that is equivalent to an aperiodic and irreducible Markov Chain. \nGiven an initial set of improvements N, we simply remove an improvement on one of the improved edges and add an improvement on one of the non-improved edges. Note that with this update procedure, we have Psi(N N) = Psi(N N) for all N and N. So we can focus only on the exponential terms. ","category":"page"},{"location":"optimal_transport.html","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"That's really all we need to solve for the optimal policy using the Metropolitan-Hastings algorithm. ","category":"page"},{"location":"other_mh.html#Other-examples-of-MH-algorithms","page":"Exploration of other MH Algorithms","title":"Other examples of MH algorithms","text":"","category":"section"},{"location":"other_mh.html#R","page":"Exploration of other MH Algorithms","title":"R","text":"","category":"section"},{"location":"other_mh.html#A-short-primer-from-a-[blog](https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/)","page":"Exploration of other MH Algorithms","title":"A short primer from a blog","text":"","category":"section"},{"location":"other_mh.html#Setup","page":"Exploration of other MH Algorithms","title":"Setup","text":"","category":"section"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"Note: A lot of this is copied verbatim from the blog post. This should be deleted before we make anything public. ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"Lets say you have a distribution p(x) that you can characterize explicitly (more or less), but don't know how to sample from. Say its","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"p(x) = fracexp(-x^2)(2 + sin(5x) + sin(2x))int_-infty^infty exp(-u^2)(2 + sin(5u) + sin(2u))  du","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"We don't know how to solve the integral in the denominator, or we are too lazy to try. So decide to work with","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"p(x) propto exp(-x^2)(2 + sin(5x) + sin(2x))","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"The basic idea is to define a Markov chain over possible x values in such a way that the stationary distribution of the Markov chain is in fact p(x). That is, where going to usee a Markov chain to generate sequences of x values, denoted (x_0 x_1  x_n) in such a way that as n rightarrow infty, we can guarantee that x_n sim p(x). ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"Suppose the current state of the Markov chain is x_n and we want to generate state x_n+1. In the Metropolitan-Hasings algorithm, the generation of x_n+1 is a two-state process. ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"The first state is to generate a  candidate, which we will denote x^*. This is a distribution we already know how to sample from (conditional on current state x_n). Denote this distribution q(x^* mid x_n). A typical way to do this is by assuming normality","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"x^* mid x_n sim textNormal(x_n sigma^2)","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"where sigma is something we select in advance.","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"The second step is the accept-reject step. First, we need to calculate the acceptance probability, denoted A(x_n rightarrow x^*), which is given","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"A(x_n rightarrow x^*) = min left(1 fracp(x^*)p(x^n) times fracq(x_n mid x^*)q(x^* mid x_n) right)","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"Two things to note. First, notice the ratio p(x^*)  p(x^n) doesn't depend on the normalizing constant for the distribution. This helps us avoid solving complicated integrals analytically or numerically. ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"Second, look at the term ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"fracq(x_n mid x^*)q(x^* mid x_n)","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"this helps correct for any biases the proposal distribution might induce. When ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"q(x_n \\mid x^*) = q(x^* \\mid x_n)","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"that is, when the candidate Markov chain is reversible, then this is referred to as the Metropolis algorithm (no Hastings, I guess.)","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"Having proposed the calculated acceptance probability A(x_n rightarrow x^*), we then select x_n+1 by choosing randomly between the two. Draw a value u randomly between 0 and 1 from a uniform distribution.","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"x_n+1 = begincases\nx^*  textif  u leq A(x_n rightarrow x^*) \nx_n  textotherwise\nendcases","category":"page"},{"location":"other_mh.html#Implementing-the-sampler","page":"Exploration of other MH Algorithms","title":"Implementing the sampler","text":"","category":"section"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"See scratch/R/blog.R","category":"page"},{"location":"other_mh.html#Additional-notes","page":"Exploration of other MH Algorithms","title":"Additional notes","text":"","category":"section"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"There is a trade-off between acceptance rate and mixing rate. Focusing on sigma, the dispersion parameter in the candidate distribution. If sigma is too narrow, the sampler will have an extremely high acceptance rate, on average, but it will move around very slowly, i.e. have a very low mixing rate. ","category":"page"},{"location":"other_mh.html","page":"Exploration of other MH Algorithms","title":"Exploration of other MH Algorithms","text":"The lag parameter is important. The lag parameter is also called the \"thinning rate.\" In some situations, you can be forced into using a proposal distribution that has a very low acceptance rate. This means you can have an algorithm get stuck in one location for long periods of time. To counteract this, you allow several iterations of the sampler to elapse in between successive samples. You give the algorithm many different attempts to reject the current value. In essence, this is reducing the autocorrelation between successive samples in our chain. ","category":"page"},{"location":"math.html#Mathematical-Appendix","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"","category":"section"},{"location":"math.html#The-Standard-Optimal-Policy-Problem","page":"Mathematical Appendix","title":"The Standard Optimal Policy Problem","text":"","category":"section"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"The researcher starts with model economy which is parametrized according to estimated parameters hattheta. Denote N in mathcalN to be a \"policy\" within the the model. For any given pair of parameter estimates (hattheta N) we can generate a welfare level W(N hattheta) in mathbbR. In this way, we can compare policies N N in mathcalN. ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"For example, the consider a model of the transportation throughout the city, where hattheta represents, among other things, the preferences of households for taking the car vs. the bus, or the speed of various transportation modes. In this example, mathbbN represents the set of public transportation routes throughout the city. ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"The researcher wishes to advise the policy-maker on the optimal policy. That is, they want to find","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"N^* in arg max_N W(N hattheta)","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Consider the case where N is high-dimensional or otherwise difficult to characterize and where mathcalN either a very large discrete set or an otherwise large continuous space. In this scenario, we face two main constraints. ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"he high dimensionality of N can make conventional descent-based optimization methods prohibitively computationally expensive. \nThe large state space of mathcalN makes it difficult to ensure the we have correctly identified the global best policy N^* as opposed to one of many local optima. ","category":"page"},{"location":"math.html#The-Relaxed-Optimal-Policy-Problem","page":"Mathematical Appendix","title":"The Relaxed Optimal Policy Problem","text":"","category":"section"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"To solve these issues, we re-characterize the optimal policy problem such that our optimal policy N^* is now defined as","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"N^* in arg max W(N hattheta) + epsilon_N","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"where epsilon_N is an i.i.d extreme value type-I distribution with dispersion parameter beta. That is, a textGumble(beta^-1).","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"In this relaxed problem, epsilon_N is unobserved by the researcher. This might represent, for example, social welfare factors that are not in the model, or idiosyncratic policy effects that are not observed by the researcher. In the relaxed problem, beta, the dispersion of errors epsilon_N is also unobserved by the researcher and the researcher must make assumptions about its value. ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"This re-characterization of the optimal problem now implies that, from the researcher's perspective, any policy N in mathcalN could be the optimal policy N^*, given a high enough unobserved value of the idiosyncratic shock epsilon_N. As a consequence, the researcher is no longer in searching for the single optimal policy, now seeks to characterize policies by the probability that a given policy is optimal. ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Given standard results about multinomial logit probabilities, we define the probability that a given policy N is optimal as","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"mathbbP(N textoptimal) = pi_beta(N) = fracexpbeta W(N hattheta)sum_N in mathcalN expbeta W(N hattheta)","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Our goal, then, is to estimate pi_beta(N). ","category":"page"},{"location":"math.html#The-Metropolis-Hastings-Algorithm-for-Calculating-Optimal-Policies","page":"Mathematical Appendix","title":"The Metropolis-Hastings Algorithm for Calculating Optimal Policies","text":"","category":"section"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Given our constraints, listed above, that N is both high dimensional and mathcalN is a large set, it is intractable to compute or estimate the probabilities pi_beta(N) explicitly. To compensate for this, instead of analyzing features of pi_beta(N), we generate a set of \"likely optimal policies\"","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Instead, our goal is to sample optimal policies from the distribution pi_beta(N) without fully characterizing pi_beta(N). We accomplish this through the Metropolitan-Hastings algorithm. ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"At a high level, the Metropolitan-Hastings algorithm is a Markov Chain Monte-Carlo algorithm for sampling from a probability distribution which is difficult to characterize. It is an iterative procedure which takes as an input an initial \"guess\" of an initial Markov Chain and over time will characterize a Markov Chain whose stationary distribution corresponds to the distribution of interest. The exposition of this section borrows from Levin and Peres (2017).","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Fixing ideas, consider an initial Markov chain Psi which is both aperiodic and irreducible, and satisfies Psi(N N)  0 iff Psi(N N)  0. Begin with network N_1. Given network N_k and step k, draw a candidate network N from the initial distribution Psi(N mid N_k). This candidate network N becomes N_k+1 with probability given","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"mathbbP(N_k+1 = N mid N_k) = min left(1 fracexp(beta W(N))Psi(N_k mid N)exp(beta W(N_k))Psi(N mid N_k)right)","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"To understand this expression, examine the case where Psi(N N) = 1 text for all  N N in mathcalN. If W(N)  W(N), then N is accepted. If W(N)  W(N), then it is accepted with a probability that is increasing in W(N). ","category":"page"},{"location":"math.html","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"As k rightarrow infty, then mathbbP(N_k = N) rightarrow pi_beta(N).  ","category":"page"},{"location":"index.html#High-Dimensional-Optimal-Policies","page":"Introduction","title":"High Dimensional Optimal Policies","text":"","category":"section"}]
}
