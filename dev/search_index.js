var documenterSearchIndex = {"docs":
[{"location":"references/","page":"References","title":"References","text":"G. Kreindler, A. Gaduh, T. Graff, R. Hanna and B. A. Olken. Optimal Public Transportation Networks: Evidence from the World's Largest Bus Rapid Transit System in Jakarta. Working Paper 31369 (National Bureau of Economic Research, Jun 2023).\n\n\n\nS. Syed, A. Bouchard-Côté, G. Deligiannidis and A. Doucet. Non-reversible parallel tempering: A scalable highly parallel MCMC scheme. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 84, 321–350 (2022).\n\n\n\nT. Allen and C. Arkolakis. The Welfare Effects of Transportation Infrastructure Improvements. The Review of Economic Studies 89 (2022).\n\n\n\n","category":"page"},{"location":"optimal_transport/#Example:-Optimal-Transportation-Policy","page":"Optimal Transport Example","title":"Example: Optimal Transportation Policy","text":"","category":"section"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"This document outlines the set-up for the optimal transport problem  implemented in the examples folder. It shows a more serious  mathematical problem that is a good candidate for this package. ","category":"page"},{"location":"optimal_transport/#Set-up","page":"Optimal Transport Example","title":"Set-up","text":"","category":"section"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Consider a city defined as a weighted graph with N nodes and a road network defined as E edges (roads) between nodes. The weight of the graph represents travel time between nodes. If nodes i and j have a road connecting them, the travel time for the direct route between the two nodes is given by t_ij. If nodes i and j do not have a road connecting them, then there is no direct route and a traveler is forced to take an indirect route (however an indirect route may still be faster than a direct route).","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"An individual lives in node i and works in node j. All home and workplace decisions are exogenous. Denote the fraction of individuals commuting from location i to location j as p_ij.","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"However an individual still chooses the route they take from home to work. That is, they choose what node-to-node-to-node route mathfrakR_ij they take between their home i and work j. ","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"We borrow the decision problem and solution for a worker's optimal route from Allen and Arkolakis [3]. In brief, workers seek the route with the shortest travel time, but also receive i.i.d. Frechet preference shocks over potential routes.","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Allen and Arkolakis [3] show that for a given i-j home-workplace pair, the effective expected travel cost faced by the worker, accounting for both travel time and the idiosyncratic preference shock, can be expressed as ","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"tau_ij = left(sum_r in mathfrakR_ij left(prod_l = 1^L t^-theta_r_l-1 r_lright)right)^-frac1theta","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Where t_r_l-1 rl corresponds to the node-to-node travel time between leg l-1 and step l on the route. ","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Moreover Allen and Arkolakis show that there is a convenient matrix expression for this travel time as well. Let mathbbA = t_ij^-theta, then define mathbfB as the Leontief inverse of A","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"mathbfB = (I - A)^-1","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"and we thus have","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"tau_ij = b_ij^-frac1theta","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"The planner cares about the total cost of travel faced by all individuals in the city. Thus their welfare function is","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Wleft(tau_ij_ijin Nright) = -left(sum_ij in Ntau_ijpi_ijright)","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"Let mathbfT be an N times N matrix representing the travel time between any two nodes, and let alpha be the shape parameter of the idiosyncratic Frechet preference shock for any route.","category":"page"},{"location":"optimal_transport/#The-Policy-Space","page":"Optimal Transport Example","title":"The Policy Space","text":"","category":"section"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"The planner has the budget to improve K  E roads between nodes by reducing travel speed on a given road. They need to choose which roads they want to upgrade. ","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"We find the best combination of roads to upgrade through the  Metropolitan-Hastings algorithm. Mapping the problem described above to the discussion previously about the Metropolitan-Hastings algorithm we have","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"mathcalN: The policy space. This is a list of K roads to improve. Note that this policy space is very large, exactly N text Choose  K. If you have 40 roads and have the budget to improve 10 of them, you have approximately 848 million ways to do that. \nPsi(N N): The initial guess for a Markov Chain. Because the  policy space is so large, we don't characterize Psi directly,  and instead think of a method for drawing N given N in a way that is equivalent to an aperiodic and irreducible Markov Chain. \nGiven an initial set of improvements N, we simply remove an improvement on one of the improved edges and add an improvement on one of the non-improved edges. Note that with this update procedure, we have Psi(N N) = Psi(N N) for all N and N. So we can focus only on the exponential terms. ","category":"page"},{"location":"optimal_transport/","page":"Optimal Transport Example","title":"Optimal Transport Example","text":"That's really all we need to solve for the optimal policy using the Metropolitan-Hastings algorithm. ","category":"page"},{"location":"math/#Mathematical-Appendix","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"","category":"section"},{"location":"math/#The-Standard-Optimal-Policy-Problem","page":"Mathematical Appendix","title":"The Standard Optimal Policy Problem","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"The researcher starts with model economy which is parametrized according to estimated parameters hattheta. Denote N in mathcalN to be a \"policy\" within the the model. For any given pair of parameter estimates (hattheta N) we can generate a welfare level W(N hattheta) in mathbbR. In this way, we can compare policies N N in mathcalN. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"For example, the consider a model of the transportation throughout the city, where hattheta represents, among other things, the preferences of households for taking the car vs. the bus, or the speed of various transportation modes. In this example, mathbbN represents the set of public transportation routes throughout the city. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"The researcher wishes to advise the policy-maker on the optimal policy. That is, they want to find","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"N^* in arg max_N W(N hattheta)","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Consider the case where N is high-dimensional or otherwise difficult to characterize and where mathcalN either a very large discrete set or an otherwise large continuous space. In this scenario, we face two main constraints. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"he high dimensionality of N can make conventional descent-based optimization methods prohibitively computationally expensive. \nThe large state space of mathcalN makes it difficult to ensure the we have correctly identified the global best policy N^* as opposed to one of many local optima. ","category":"page"},{"location":"math/#The-Relaxed-Optimal-Policy-Problem","page":"Mathematical Appendix","title":"The Relaxed Optimal Policy Problem","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"To solve these issues, we re-characterize the optimal policy problem such that our optimal policy N^* is now defined as","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"N^* in arg max W(N hattheta) + epsilon_N","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"where epsilon_N is an i.i.d extreme value type-I distribution with dispersion parameter beta. That is, a textGumble(beta^-1).","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"In this relaxed problem, epsilon_N is unobserved by the researcher. This might represent, for example, social welfare factors that are not in the model, or idiosyncratic policy effects that are not observed by the researcher. In the relaxed problem, beta, the dispersion of errors epsilon_N is also unobserved by the researcher and the researcher must make assumptions about its value. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"This re-characterization of the optimal problem now implies that, from the researcher's perspective, any policy N in mathcalN could be the optimal policy N^*, given a high enough unobserved value of the idiosyncratic shock epsilon_N. As a consequence, the researcher is no longer in searching for the single optimal policy, now seeks to characterize policies by the probability that a given policy is optimal. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Given standard results about multinomial logit probabilities, we define the probability that a given policy N is optimal as","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"mathbbP(N textoptimal) = pi_beta(N) = fracexpbeta W(N hattheta)sum_N in mathcalN expbeta W(N hattheta)","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Our goal, then, is to estimate pi_beta(N). ","category":"page"},{"location":"math/#The-Metropolis-Hastings-Algorithm-for-Calculating-Optimal-Policies","page":"Mathematical Appendix","title":"The Metropolis-Hastings Algorithm for Calculating Optimal Policies","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Given our constraints, listed above, that N is both high dimensional and mathcalN is a large set, it is intractable to compute or estimate the probabilities pi_beta(N) explicitly. To compensate for this, instead of analyzing features of pi_beta(N), we generate a set of \"likely optimal policies\"","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Instead, our goal is to sample optimal policies from the distribution pi_beta(N) without fully characterizing pi_beta(N). We accomplish this through the Metropolitan-Hastings algorithm. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"At a high level, the Metropolitan-Hastings algorithm is a Markov Chain Monte-Carlo algorithm for sampling from a probability distribution which is difficult to characterize. It is an iterative procedure which takes as an input an initial \"guess\" of an initial Markov Chain and over time will characterize a Markov Chain whose stationary distribution corresponds to the distribution of interest. The exposition of this section borrows from Levin and Peres (2017).","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Fixing ideas, consider an initial Markov chain Psi which is both aperiodic and irreducible, and satisfies Psi(N N)  0 iff Psi(N N)  0. Begin with network N_1. Given network N_s and step s, draw a candidate network N from the initial distribution Psi(N mid N_s). This candidate network N becomes N_s+1 with probability given","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"mathbbP(N_s+1 = N mid N_s) = min left(1 fracexp(beta W(N))Psi(N_s mid N)exp(beta W(N_s))Psi(N mid N_s)right)","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"To understand this expression, examine the case where Psi(N N) = 1 text for all  N N in mathcalN. If W(N)  W(N), then N is accepted. If W(N)  W(N), then it is accepted with a probability that is increasing in W(N). ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"As s rightarrow infty, then mathbbP(N_s = N) rightarrow pi_beta(N).  ","category":"page"},{"location":"math/#Simulated-Annealing-and-Parallel-Tempering-for-Metropolis-Hastings","page":"Mathematical Appendix","title":"Simulated Annealing and Parallel Tempering for Metropolis-Hastings","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"A downside of Metropolis-Hastings algorithms a lack of \"mixing\". That is, we the algorithm may repeatedly draw the same sample over and over again. This problem is particularly severe in our context because of the functional form of our modified objective function, e^beta W(N). For high values of beta, even a marginall smaller of value of W(N) will have an almost zero chance of being accepted, and the Metropolis-Hastings algorithm will be stuck in a local optimal. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Lower values of beta will lead the Metropolis-Hastings algorithm to be less likely to stuck in a local optima. However, since marginally smaller valus of W(N) are more likely to be accepted with a small beta, meaning the algorithm will be less likely to find optimal policies at all. The trade-off is then","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"High values of beta are more likely to find optimal policies, but also more likely to get stuck in a local optima and not accurately characterize the state space of optimal policies. \nLow values of beta traverse the state space of optimal policies, but the policies it draws are less likely to be optimal in general. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"The solution to this problem is a procedure that combines low-beta and high-beta runs of the Metropolis-Hastings algorithm. Low-beta runs of the algorithm traverse the state space and search for optimal policy candidates. Later on, the high-beta runs of the algorithm take up these optimal policy guesses and draw from the state space near these candidates. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"We accomplish this general goal in two ways, with a Simulated Annealing Algorithm and with the Parallel Tempering algorithm. ","category":"page"},{"location":"math/#Simulated-Annealing","page":"Mathematical Appendix","title":"Simulated Annealing","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Simulated Annealing takes as an input a schedule of inverse temperatures beta_1  beta_2  mathellipsis  beta_K. Starting with beta_1, we run S iterations of the Metropolis-Hastings algorithm with inverse temperature beta_1. Because beta_1 is relatively low, this algorithm quickly traverses the state space and after S iterations returns a candidate network N_1. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Next, we use this candidate network N_1 as the initial state for another S runs of the Metropolis-Hastings algorithm, this time using inverse temperature beta_2. This run will traverse the state space of policies less quickly, and will spend more time on optimal policies. This run returns candidate policy N_2, and so on, until the we return policy N_k. We are confident N_k is a globally optimal policy because previous iterations of the algorithm have thoroughly explore the state space. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"To get a set of optimal policies, we run many independent runs of the Simulated Annealing algorithm. ","category":"page"},{"location":"math/#Parallel-Tempering","page":"Mathematical Appendix","title":"Parallel Tempering","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Parallel Tempering might be thought of as running simultaneous Simulated Annealing algorithms in parallel. We implement a textbook (naive) version of Parallel Tempering, as well as provide bindings to a more sophisticated version of the Algorithm, implemented by Pigeons.jl, which implements the algorithm described by Syed et al. [2].","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Like Simulated Annealing, Parallel Tempering takes as an input a schedule of inverse temperatures beta_1  beta_2  mathellipsis  beta_K. It then runs separate Metropolis-Hastings algorithms for each temperature in parallel. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"After S runs, each algorithm returns their latest policy candidate N_k and a \"swapping\" stage occurs. Inverse temperatures are \"paired up\" such that N_1 is compared to N_2, N_3 is compared to N_4, etc. For each comparison k to k+1 decide whether or not the two inverse temperatures should swap policies according to ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"mathbbP(N_k+1 = N_k N_k = N_k+1) = minleft1 expleft(left(beta_k+1 - beta_kright) times left(W(N_k) - W(N_k+1)right)right)right","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Recall that beta_k_1 - beta_k  0. So when, by chance, the lower temperature returns a more optimal policy N_k, it will be accepted. Otherwise, the lower temperature policy will be accepted according to a probability determined by the beta_k+1 - beta_k. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Including a Metropolis-Hastings swap stage after every S iterations in should result in a high degree of mixing, such that the Parallel Tempering algorithm can run indefinitely and draws from the time series of optimal policies produced by this algorithm mirror the distribution of optimal policies more generally. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"The above describes our \"naive\" implementation Parallel Tempering. Please see Syed et al. [2] for a discussion of their more sophisticated algorithm. In particular, the authors do not take the inverse temperature schedule as given. Instead, we only assign a maximum inverse temperature, N_K and the schedule is determined optimally to ensure mixing. ","category":"page"},{"location":"math/#Testing-for-optimal-mixing","page":"Mathematical Appendix","title":"Testing for optimal mixing","text":"","category":"section"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Given a set of policies N_l_l =1^L, how can we be sure that this set of policies is drawn from the the distribution of optimal policies pi_beta? We propose the following test statistic","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"hatT = hatW_max - hatW_textmean - fraclog nbeta","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Where hatW_max and hatW_textmean are the maximum and mean values of the the welfare values from the observed set of policies, W(N_l)_l =1^L, respectively, n represents the size of the state space (i.e. the number of different policies which can be selected), and beta is the inverse temperature associated with this set of draws. ","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"Asymptotically, hatT is bounded above by the normal distribution of mathcalNleft(0 fracsigma_piLright). Therefore, to test whether N_l_l =1^L are drawn independently from pi_beta, we use the one-sided t-test based on","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"frachatThatsigma_pi  sqrtL - 1","category":"page"},{"location":"math/","page":"Mathematical Appendix","title":"Mathematical Appendix","text":"where hatsigma_pi is the estimated standard deviation of welfare values.  ","category":"page"},{"location":"#High-Dimensional-Optimal-Policies","page":"Introduction","title":"High Dimensional Optimal Policies","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package implements a suite of algorithms for identifying optimal policies using the methodology of Kreindler et al. [1]. ","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"HighDimensionalOptimalPolicies.jl is not registered on the Julia registry. To download, run","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"import Pkg;\nPkg.add(\"https://github.com/pdeffebach/HighDimensionalOptimalPolicies.jl.git\")","category":"page"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This quick-start solves for the optimal policy propblem outlined in the Tutorial. Of a set of 100 prizes, which 50 do we pick? The top 50 of course! Nonetheless, given the high dimensionality of the state space, this problem is a simple illustration for how various options in the package the set of optimal policies that get chosen. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"We need three inputs, ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"initfun: A starting policy guess, of the form initfun(rng) where rng is a random number generator. \nnextfun: Given a policy guess, what is the next policy guess? Of the form nextfun(rng, state). \nobjfun: What is the objective value of this policy? ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(; initfun, nextfun, objfun) = HighDimensionalOptimalPolicies.quickstart()","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Get 200 policies drawn from the distribution of optimal policies using the get_best_policy function. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"out = get_best_policy(\n    IndependentSimulatedAnnealingSolver(); \n    initfun = initfun,\n    nextfun = nextfun, \n    objfun = objfun, \n    max_invtemp = 50.0,\n    invtemps_curvature = 2.0,\n    n_invtemps = 5,\n    n_inner_rounds = 100,\n    n_independent_runs = 200)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Get the a vector of the best policy guesses with get_policy_vec","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"get_policy_vec(out)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"get the vector of objective values with get_objective_vec","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"get_objective_vec(out)","category":"page"},{"location":"#Motivation","page":"Introduction","title":"Motivation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Policymakers are often tasked with identifying \"optimal\" policies. That is, the choices that policymakers can make which result in the highest level of welfare (however defined) for a population. Sometimes there are only a few levers available to the policymaker, such that their choice involves manipulating only a small number of key policies. For instance, \"what should the interest rate paid on bank deposits in the Federal Reserve?\" involves finding the optimal value of one key variable. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Often, however, policymakers have a multitude of levers at their disposal, and finding the \"optimal\" policy involves an optimization problem of hundreds, thousands, or even tens of thousands of different variables. For example, Kreindler et al. [1] asks what the optimal transportation network looks like in Jakarta, Indonesia. This task involves ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Where should the buses go? Along a network with 1000s of nodes and an order of magnitude more edges. \nHow many buses should go on each route? \nWhich routes should have separate Bus Rapid Transit lanes? Which should have normal lanes? ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In this instance, the state space of policy choices is so large that it is impossible to characterize a single \"best\" policy. Additionally, the problem is not convex, such that an interative procedure is unlike to find the globally optimum solution. Finally, policymakers might have other considerations not fully captured in an economist's simplified model of the economy, and would prefer a menu of candidate policies rather than one answer by itself. Finally, it may also be useful to learn what \"qualities\" are associated with a set of optimal policies rather than analyze one policy on its own. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"While optimal transportation policy is the main motivation of this package and will serve as it's guiding example, the same reasoning can be applied to any economic problem where the state space of policy levers is large, for instance place-based policies where resources are distributed across many different locations, or a tax system where taxes must be levied on a wide variety of goods. ","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here we set up a simple high dimensional problem and outline the algorithms this package provides to characterize optimal policies. ","category":"page"},{"location":"tutorial/#Set-up","page":"Tutorial","title":"Set-up","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We start by importing the HighDimensionalOptimalPolicies.jl package, along with Plots.jl, which will help us visualize results. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using HighDimensionalOptimalPolicies\nusing Plots, StatsPlots\nusing Random, Distributions\nusing LinearAlgebra, SpecialFunctions\nusing StatsBase\nusing DataFrames","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Next we define a simple high dimensional problem. Consider a vector of random numbers of length L, denoted vecr. Our goal is to find a policy vector vecp of length L filled with zeros and exactly L_p ones. We want to choose the locations of the ones to maximize","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"W(vecp) = vecp cdot vecr","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This is a convenient set-up to analyze high dimensional optimal policies because the state space is very large, L text Choose  L_p and if vecr is well chosen, then there may be many policies with similar welfare values. ","category":"page"},{"location":"tutorial/#Implementing-get_best_policy","page":"Tutorial","title":"Implementing get_best_policy","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For concreteness, we will think of L as the total number of nodes on a transportation network and L_p as the number of edges to upgrade, where we want to try and upgrade edges that have the highest value. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"n_edges = 1000\nn_edges_to_upgrade = 500\nnetwork_values = sort(rand(LogNormal(1.0), n_edges), rev = true)\nnetwork_values = network_values ./ norm(network_values)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Our goal is to call the function get_best_policy from HighDimensionalOptimalPolicies.jl. To do this, we need to define three functions ourselves","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A function for drawing initial guesses from the policy state space\nA function for drawing the next guess conditional on the current guess\nThe objective function","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"HighDimensionalOptimalPolicies.jl requires passing a random number generator (rng) to the initial-guess and next-guess functions. We use a let block to capture global variables when we define these functions in order to improve performance and reduce bugs (in case a global variable gets redefined). ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"initfun = let n_edges = n_edges, n_edges_to_upgrade = n_edges_to_upgrade\n    rng -> begin\n        fill(false, n_edges_to_upgrade)\n        inds = sample(rng, 1:n_edges, n_edges_to_upgrade; replace = false)\n        p = fill(false, n_edges)\n        p[inds] .= true\n        p\n    end\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To choose the next policy, conditional on the current one, we simply choose a pair of two policies and edges and then swap whether or not they are upgraded. That is, we randomly select an edge which is non-upgraded under the current guess and choose to upgrade it. We also randomly select an edge which is upgraded under the current guess and choose not to upgrade it. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"nextfun =  let n_edges = n_edges, n_edges_to_upgrade = n_edges_to_upgrade\n    (rng, state) -> begin\n        upgraded_edges = findall(state)\n        not_upgraded_edges = findall(==(false), state)\n\n        edge_to_drop = sample(rng, upgraded_edges)\n        edge_to_add = sample(rng, not_upgraded_edges)\n\n        new_edges_to_upgrade = copy(state)\n        new_edges_to_upgrade[edge_to_drop] = false\n        new_edges_to_upgrade[edge_to_add] = true\n\n        new_edges_to_upgrade\n    end\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, we define the objective function. Because the objective function is deterministic, we do not need to pass the random number generator to this function. However, we still use a let block to capture the transportation network values. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"objfun = let network_values = network_values\n    state -> begin\n        dot(state, network_values)\n    end\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now, we we run the Parallel Tempering algorithm to get a set of optimal policies. We specify the following arguments","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"max_invtemp: The maximum inverse temperature\ninvtemps_curvature: The way inverse temperatures \"ramp up\" from zero to the highest value (see below for deatils)\nn_invtemps: The number of inverse temperatures to use. This is synonymous with the number of \"chains\" to run with the Parallel Tempering algorithm\nn_inner_rounds: The total number of policy draws we will take\nn_swap_rounds: The number of swap rounds, i.e. the number of times the inverse temperatures \"meet up\" and randomly swap their policy states. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"out = get_best_policy(\n    PTMCMCSolver(); \n    initfun = initfun,\n    nextfun = nextfun, \n    objfun = objfun, \n    max_invtemp = 50.0,\n    invtemps_curvature = 2.0,\n    n_invtemps = 10,\n    n_inner_rounds = 5000,\n    n_swap_rounds = 100)","category":"page"},{"location":"tutorial/#The-inverse-temperatures","page":"Tutorial","title":"The inverse temperatures","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that in the above example, we did not choose the vector of inverse temperatures directly, rather we chose a maximum inverse temperature (max_invtemp) and a curvature. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This is controlled by the function make_invtemps. A value between greater than 1 of invtemps_curvature causes means many inverse temperatures are close to zero, with a slow ramp-up, while a value between 0 and 1 means many temperatures close to the maximum temperature. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nThe inverse temperatures produced by make_invtemps, and all output of get_best_policy are organized with the highest temperature first. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"max_invtemp = 25.0\nn_invtemps = 20\ninvtemps_g1 = make_invtemps(25.0, length = n_invtemps, invtemps_curvature = 2.0)\ninvtemps_1 = make_invtemps(25.0, length = n_invtemps, invtemps_curvature = 1.0)\ninvtemps_l1 = make_invtemps(25.0, length = n_invtemps, invtemps_curvature = 0.5)\nplot(1:n_invtemps, [invtemps_g1 invtemps_1 invtemps_l1]; \n    xlab = \"Inverse temperature index\",\n    ylab = \"Inverse temperature\", \n    label = [\"Curvature = 2.0\" \"Curvature = 1.0\" \"Curvature = 0.5\"])","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In general, you want to use an invtemps_curvature greater than 1 to ensure sufficient mixing. ","category":"page"},{"location":"tutorial/#Exploring-the-output","page":"Tutorial","title":"Exploring the output","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We use the functions get_objective_vec and get_policy_vec and to inspect our output. To start, let's plot out objective values across time to see how the algorithm converged from drawing uniformly from the state space of policies to drawing from the distribution of optimal policies. We use the keyword argument only_last_half to tell get_objective_vec that we want the objective value from all iterations, including the initial burn-in period which are unlikely to be optimal draws. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here we see that in the last half of the iterations, the objective value resembles a random walk, indicating we have settled on a set of optimal policies. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function plot_objectives_time(out; only_last_half = false, ind = 1)\n    objvec = get_objective_vec(out; only_last_half, ind)\n    plot(\n        1:length(objvec), \n        objvec, \n        xlab = \"Iteration\", \n        ylab = \"Objective value\", \n        label = false, \n        color = \"black\")\nend\n\nplot_objectives_time(out)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"What can we learn about the optimal policies? Recall that our vector of network values was sorted from the highest value to the lowest. As a consequence, we should see lots of improved edges close close to the front of the vector. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here we see that edges close to the front of the vector are almost always improved, while those close to the back of the vector are almost never improved. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this example, we know that the optimal policy in general is to improve the first 100 edges. However we clearly are not stuck in this global optimum, because at index 100, edges are being improved at far less than 100% of the time. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"plot_mean_policy = function(out)\n    pvec = get_policy_vec(out)\n    # You can also do get_average_policy(out)\n    mean_policy = mean(pvec)\n    plot(\n        1:length(mean_policy),\n        mean_policy .* 100,\n        xlab = \"Edge index\",\n        ylab = \"Percent of iterations with improvement\",\n        label = false, \n        color = \"black\", \n        xticks = 0:100:n_edges, \n        ylim = (0, 100))\nend\nplot_mean_policy(out)","category":"page"},{"location":"tutorial/#Testing-for-optimal-mixing","page":"Tutorial","title":"Testing for optimal mixing","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The function test_mixing returns a t-statistic for whether the objective values drawn using our algorithm are drawn from the set of optimal policies. To use this function, however, we need to account for the log of the size of the state space, log_n. For our current example, 1000 text Choose  100 is such a large number Julia cannot calculate it without an overflow! We use the function logabsbinomial from SpecialFunctions.jl instead, which gives us the log of the size of the state space for this problem. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For more information, see Kreindler et al. [1]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"log_n = SpecialFunctions.logabsbinomial(n_edges, n_edges_to_upgrade)[1]\ntest_mixing(out, log_n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This low value indicates we are highly confident that we are drawing from the the optimal set of policies. ","category":"page"},{"location":"tutorial/#Parallelization","page":"Tutorial","title":"Parallelization","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The algorithm PTMCMCSolver uses the Distributed standard library's pmap for parallelization, so parallalelization happens automatically depending on the number of cores Julia is running with. ","category":"page"},{"location":"tutorial/#Parallel-Tempering-with-Pigeons.jl","page":"Tutorial","title":"Parallel Tempering with Pigeons.jl","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In addition to our \"naive\" implementation of Parallel Tempering, we also provide an interface for the Parallel Tempering algorithm of Syed et al. [2], as implemented by Pigeons.jl. To use Pigeons.jl, we use the PigeonsSolver(). We also omit the keyword argument invtemps_curvature, because the algorithm optimally chooses the annealing schedule, and the keyword argument n_swap_rounds, because the number of swap rounds is deterministically set by the number of rounds.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"out_pigeons = get_best_policy(\n    PigeonsSolver(); \n    initfun = initfun,\n    nextfun = nextfun, \n    objfun = objfun, \n    max_invtemp = 50.0,\n    n_invtemps = 6,\n    n_inner_rounds = 5000)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can compare the optimally chosen inverse temperatures chosen by Pigeons.jl with the ones we created using invtemps_curvaturre","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"println(get_invtemps(out))\nprintln(get_invtemps(out_pigeons))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"And compare the average policy with the output using Pigeons.jl","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"plot_mean_policy(out_pigeons)","category":"page"},{"location":"tutorial/#Parallelization-with-Pigeons.jl","page":"Tutorial","title":"Parallelization with Pigeons.jl","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pigeons.jl does not use the Distributed standard library for parallelization. Rather, it uses the MPI distributed computing protocol to spawn sub-processes, and then aggregates those sub-processes together manually. To use this, you must have MPI installed on your machine. See instructions here to download MPI on Linux. On a computing cluster, you may also need to initialiate your session with MPI-related flags. See here for instructions on Boston University's computing cluster. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To use Pigeons.jl's parallelization, we use the PigeonsMPISolver() solver, and specify the number of child processes to spawn. If you have additional dependencies needed for your initfun, nextfun, and objfun to work, besides your Main module and HighDimensionalOptimalPolicies, you need to pass these dependencies to each child process using the dependencies keyword argument. You must also have Pigeons.jl installed in your current environment. For instance, we use StatsBase.sample along with LinearAlgebra.dot, so we pass these modules as dependencies. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"out_pigeons_mpi = get_best_policy(\n    PigeonsMPISolver(); \n    initfun = initfun,\n    nextfun = nextfun, \n    objfun = objfun, \n    max_invtemp = 50.0,\n    n_invtemps = 10,\n    n_inner_rounds = 100,\n    n_local_mpi_processes = 2,\n    n_threads = 2, \n    dependencies = [StatsBase, LinearAlgebra])","category":"page"},{"location":"tutorial/#Independent-Simulated-Annealing-runs","page":"Tutorial","title":"Independent Simulated Annealing runs","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"An alternative to Parallel Tempering is to simply run many independent runs of a Simulated Annealing algorithm. We do this with the IndependentSimulatedAnnealingSolver solver. Here n_inner_rounds refers to the number of Metropolis-Hastings steps for a given temperature","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"out_sa = get_best_policy(\n    IndependentSimulatedAnnealingSolver(); \n    initfun = initfun,\n    nextfun = nextfun, \n    objfun = objfun, \n    max_invtemp = 50.0,\n    invtemps_curvature = 2.0,\n    n_invtemps = 10,\n    n_inner_rounds = 1000,\n    n_independent_runs = 200)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that the average policy shows a smoother decay from the start of the edge vector to the end. This is because, since simulated annealing runs are independent, the algorithm does not get stuck improving particular edges for a long amount of time, as occurs in Parallel Tempering, where policies are generated using an (ergodic) sequence of Metropolis-Hastings draws. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"plot_mean_policy(out_sa)","category":"page"},{"location":"tutorial/#Comparing-inverse-temperatures","page":"Tutorial","title":"Comparing inverse temperatures","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As discussed above (and in the mathematical appendix), there is a tension between mixing and optimality. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function plot_all_objectives(out)\n    p = plot()\n    invtemps = get_invtemps(out)\n\n    for ind in 1:(length(invtemps))\n        obj = get_objective_vec(out; ind)\n        label = invtemps[ind]\n        density!(p, obj; label = label, line_z = invtemps[ind], zcolor = invtemps[ind], palette = cgrad(:grays))\n    end\n    p\nend\nplot_all_objectives(out)","category":"page"},{"location":"tutorial/#Saving-and-Reading-Across-Multiple-Independent-Runs","page":"Tutorial","title":"Saving and Reading Across Multiple Independent Runs","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Running many solvers in parallel across multiple compute jobs is a time-efficient way to draw a large number of optimal policies. For example, you may want to run an array job in slurm where you run the exact same analysis in parallel. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We provide utilities to save the policy guesses and objective values in a consistent way and read in these values across many independent runs, even if the runs were on separate computing jobs entirely. ","category":"page"},{"location":"tutorial/#Running-Multiple-Independent-Runs-using-Slurm","page":"Tutorial","title":"Running Multiple Independent Runs using Slurm","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Imagine you have written a package called MyAnalysisPackage.jl which has HighDimensionalOptimalPolicies.jl as a dependency to analyze a specific policy. ","category":"page"},{"location":"tutorial/#Saving-output","page":"Tutorial","title":"Saving output","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For any given run, we can save outputs with the save_policy_output_csv function. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"mkdir(\"tmp_output\")\nfor i in 1:3\n    out_temp = get_best_policy(\n        PTMCMCSolver(); \n        initfun = initfun,\n        nextfun = nextfun, \n        objfun = objfun, \n        max_invtemp = 50.0,\n        invtemps_curvature = 2.0,\n        n_invtemps = 6,\n        n_inner_rounds = 5000,\n        n_swap_rounds = 100)\n\n    save_policy_output_csv(out_temp; outdir = \"tmp_output\", only_max_invtemp = true)\nend","category":"page"},{"location":"tutorial/#Reading-Output","page":"Tutorial","title":"Reading Output","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The MultiCSVPolicyOutput type reads in all .csv files in a given  directory and stores them so that you can access the vector of policy guesses and objective values just like you can with other output (i.e. out and out_pigeons in this tutorial). ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"warning: Warning\nsave_policy_output_csv does not validate inputs on writing, and MultiCSVPolicyOutput does not validate inputs on reading. It is up to the user to ensure that all inputs to the solver are the exact same for all .csv files saved.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"MultiCSVPolicyOutput is a limited object. Unlike other policy ouputs (MultiMCMCPolicyOutput etc.) it does not store the underlying functions initfun, nextfun, or objfun. It is only useful for analyzing results, ideally in a session where the exact same initfun, nextfun, or objfun that created the .csv files you read in. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"waning: Waning\nThis function, and the workflow using .csv in general, is highly un-optimized. When policies are large (100+ elements), then the .csv files saved will be large and reading the files in has the potential to crash your computer with an out-of-memory errory. This is best used on the cluster when memory constraints are less severe. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"out_csv = MultiCSVPolicyOutput(\"tmp_output\")","category":"page"},{"location":"tutorial/#Running-Independent-Jobs-using-Slurm","page":"Tutorial","title":"Running Independent Jobs using Slurm","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"When starting an Array job on the cluster, I highly recommend you build a System Image of all the Julia packages you will use when evaluating your policy. For instructions, see documentation here. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Below is an example of using the -t option to run an an array job using Slurm by starting multiple independent compute processes. Taken from Boston University's tutorials here. This bash script will initiate 25 independent jobs on a machine with 1 core. We can alter the number of cores and tasks depending on the computing resources necessary. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"#!/bin/bash -l\n\n# Specify that we will be running an Array job with 25 tasks numbered 1-25\n#$ -t 1-25\n\n# Request 1 core for my job\n#$ -pe omp 1\n\n# Give a name to my job\n#$ -N optimal_policies\n\n# Join the output and error streams\n#$ -j y\n\n# Run my julia script \njulia myscript.jl","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Your myscript.jl might look something like this","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using HighDimensionalOptimalPolicies\nusing MyAnalysisPackage\n\nout = get_best_policy(...)\nsave_policy_output_csv(oudir = \"tmp_output\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Then in an additional session, you can read in the results saved by the various tasks in this array job with by calling MultiCSVPolicyOutput(\"tmp_output\"). ","category":"page"},{"location":"tutorial/#Casting-Policy-Outputs-to-DataFrames-for-Manual-Saving","page":"Tutorial","title":"Casting Policy Outputs to DataFrames for Manual Saving","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To convert to DataFrames, use the function Tables.dictcolumntable as an intermediate tabular representation of a policy output.  ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"df = DataFrame(Tables.dictcolumntable(out))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We use an intermediate representation because HighDimensionalOptimalPolicies.jl does not have DataFrames.jl as a dependency. ","category":"page"},{"location":"tutorial/#Running-Many-Parralel-Jobs-within-the-same-Julia-session","page":"Tutorial","title":"Running Many Parralel Jobs within the same Julia session","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All solvers in HighDimensionalOptimalPolicies.jl use pmap internally, meaning running processes in parrallel should \"just work\". ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"When we run the independent Simulated Annealing-based optimal policy draws using the IndependentSimulatedAnnealingSolver type, the separate simulated annealing process are spread across multiple cores. By contrast, when we run Parallel Tempering, chains (i.e. MCMC runs for each inverse temperature) are spread across multiple cores. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Parallelization with Pigeons.jl, using the PigeonsMPISolver method, is more complicated and requires access to MPI. See Pigeons.jl documentation for more details. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A script using many cores in the same Julia session might look like ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Distributed\n\n@everywhere using HighDimensionalOptimalPolicies\n@everywhere using Distributions\n@everywhere using Random\n@everywhere using LinearAlgebra\n\n# Use addprocs if you did not start Julia with multiple\n# processes at the command line\n# addprocs()\n\n# Use a large number of edges\ninitfun, nextfun, objfun = HighDimensionalOptimalPolicies.quickstart(1000)\n\nout_par = get_best_policy(\n    IndependentSimulatedAnnealingSolver(); \n    initfun = initfun,\n    nextfun = nextfun, \n    objfun = objfun, \n    max_invtemp = 50.0,\n    invtemps_curvature = 2.0,\n    n_invtemps = 10,\n    n_independent_runs = 500)\n\nprintln(\"Successfull finish\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"And a qsub script which submits a job with a given number of cores might look like, where the variable $NSLOTS is created automatically by Slurm, which represents the number of cores available to Julia. We use the $NSLOTS variable to start Julia with the appropriate number of cores. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Consult with your local research computing staff for how to start a job with 100s or even 1000s of nodes in the same Julia process. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"#!/bin/bash -l\n\n# Request a computer with 16 cores\n#$ -pe omp 16\n\n# Choose cores with 8 GB of memory\n#$ -l mem_per_core=8G\n\n# Give a name to my job\n#$ -N optimal_policies\n\n# Join the output and error streams\n#$ -j y\n\n#$ -l h_rt=04:00:00\n\necho $NSLOTS\njulia --project -p $NSLOTS script.jl","category":"page"}]
}
